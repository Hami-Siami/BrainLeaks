{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$$ MI \\ Attack \\ Evaluation - MNIST - SNN - BrainLeaks $$"
      ],
      "metadata": {
        "id": "uif3m0pbONbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports"
      ],
      "metadata": {
        "id": "Ou1YDt6SGKO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSbT0sH6GRkB",
        "outputId": "763406fb-9a1c-4a94-c114-dc14b1622a3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.23.5)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.1 nirtorch-1.0 snntorch-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bbt6ZsPzTu7_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import utils\n",
        "from snntorch import spikegen\n",
        "import snntorch.spikeplot as splt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "ApDD36VBNtgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Architecture\n",
        "\n",
        "num_inputs= 28*28\n",
        "num_outputs = 10\n",
        "batch_size=100\n",
        "\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 25      # Number of Time-Steps for Encoding the Static Input\n",
        "beta = 0.7          # Leakage (Decay) Factor of LIF Neurons\n",
        "spike_grad=surrogate.fast_sigmoid(slope=40) # surrogate function\n",
        "\n",
        "# Other\n",
        "data_path='/data/mnist'\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "aBK_qVhQtWuN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test  = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "H9cMaKWzHT4m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Spiking Neural Network Model (Evaluation Classifier)"
      ],
      "metadata": {
        "id": "k8vMZsyMNyK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(12, 24, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(24 * 4 * 4, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ],
      "metadata": {
        "id": "xNxjsoUPtPrC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(net, num_steps, data):\n",
        "  mem_rec = []\n",
        "\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      _, mem_out = net(data[step])\n",
        "\n",
        "      mem_rec.append(mem_out)\n",
        "\n",
        "  return  torch.stack(mem_rec)"
      ],
      "metadata": {
        "id": "-2XBchiMvrVr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to print metrics during training loop\n",
        "\n",
        "def print_batch_accuracy(data, targets, train=False):\n",
        "\n",
        "\n",
        "    output = forward_pass(net, num_steps, data)\n",
        "    idx = output.sum(dim=0).argmax(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train Set Accuracy: {acc}\")\n",
        "    else:\n",
        "        print(f\"Test Set Accuracy: {acc}\")\n",
        "\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
        "    print_batch_accuracy(spike_data, targets_it, train=True)\n",
        "    print_batch_accuracy(test_spike_data, testtargets_it, train=False)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "5SKipYdXvs9G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The state dictionary containing the pre-trained model's learned parameters is saved in **'MNIST_SNN_Weights_Eval'** and can be loaded by running the following cell."
      ],
      "metadata": {
        "id": "PnVvp9oqMvCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"/content/MNIST_SNN_Weights_Eval\",map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L3o7AwQQ-9o",
        "outputId": "caaec62c-99fd-43ca-b103-d171b32d10b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following cell contains the training loop. If you have already loaded learned parameters from \"MNIST_SNN_Weights_Eval\", you don't need to run this cell."
      ],
      "metadata": {
        "id": "dJ1mQ3svM_nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Adam optimizer for training the neural network with a specified learning rate and betas.\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
        "\n",
        "# Instantiate the log softmax function and the negative log-likelihood loss function.\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "# Initialize lists to store training and testing loss values.\n",
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "\n",
        "counter = 0\n",
        "num_steps=25\n",
        "# Outer training loop\n",
        "for epoch in range(2): # These are the last two epochs to show the loss and metrics through the training\n",
        "    minibatch_counter = 0\n",
        "    data = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in data:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # Spike generator\n",
        "        spike_data = spikegen.rate(data_it, num_steps)\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        mem_rec = forward_pass(net, num_steps, spike_data)\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "        # print(loss_val)\n",
        "        # break\n",
        "        # Gradient Calculation\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "\n",
        "        # Weight Update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store Loss history\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set spike conversion\n",
        "        test_spike_data = spikegen.rate(testdata_it,num_steps)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_mem_rec = forward_pass(net, num_steps, test_spike_data)\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 100 == 0:\n",
        "            train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cju4qb2xv0Ej",
        "outputId": "929ab088-1eaa-4303-ab21-e263df4cf27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 2.0587158203125\n",
            "Test Set Loss: 2.363649845123291\n",
            "Train Set Accuracy: 0.98\n",
            "Test Set Accuracy: 0.99\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 1.8622440099716187\n",
            "Test Set Loss: 2.044363260269165\n",
            "Train Set Accuracy: 0.97\n",
            "Test Set Accuracy: 0.99\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 1.8321194648742676\n",
            "Test Set Loss: 0.732135534286499\n",
            "Train Set Accuracy: 0.99\n",
            "Test Set Accuracy: 1.0\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 2.3062918186187744\n",
            "Test Set Loss: 1.1230648756027222\n",
            "Train Set Accuracy: 0.98\n",
            "Test Set Accuracy: 1.0\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 1.0212091207504272\n",
            "Test Set Loss: 2.224919080734253\n",
            "Train Set Accuracy: 1.0\n",
            "Test Set Accuracy: 0.96\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 500\n",
            "Train Set Loss: 1.2539761066436768\n",
            "Test Set Loss: 1.781040072441101\n",
            "Train Set Accuracy: 0.99\n",
            "Test Set Accuracy: 0.98\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 0\n",
            "Train Set Loss: 2.6294121742248535\n",
            "Test Set Loss: 1.7578938007354736\n",
            "Train Set Accuracy: 0.98\n",
            "Test Set Accuracy: 0.99\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 100\n",
            "Train Set Loss: 0.9987372159957886\n",
            "Test Set Loss: 1.7149631977081299\n",
            "Train Set Accuracy: 1.0\n",
            "Test Set Accuracy: 0.99\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 200\n",
            "Train Set Loss: 1.9482544660568237\n",
            "Test Set Loss: 0.8164709210395813\n",
            "Train Set Accuracy: 0.97\n",
            "Test Set Accuracy: 1.0\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 300\n",
            "Train Set Loss: 1.1524057388305664\n",
            "Test Set Loss: 1.6312106847763062\n",
            "Train Set Accuracy: 1.0\n",
            "Test Set Accuracy: 0.98\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 400\n",
            "Train Set Loss: 0.9207310080528259\n",
            "Test Set Loss: 1.490919589996338\n",
            "Train Set Accuracy: 1.0\n",
            "Test Set Accuracy: 0.97\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 500\n",
            "Train Set Loss: 1.0291166305541992\n",
            "Test Set Loss: 2.542302131652832\n",
            "Train Set Accuracy: 0.97\n",
            "Test Set Accuracy: 0.96\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(net.state_dict(),'MNIST_SNN_Weights_Eval')"
      ],
      "metadata": {
        "id": "VrFHTbS4wGbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "TcZdSABeN9Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        spike_data = spikegen.rate(data,num_steps=num_steps)\n",
        "\n",
        "        output = forward_pass(net, num_steps, spike_data)\n",
        "        predicted = output.sum(dim=0).argmax(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b-1Q9MKH7ua",
        "outputId": "63835c23-4d49-4081-fb03-32b6ef5d8d80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attack Evaluation Phase"
      ],
      "metadata": {
        "id": "Xua_k3IHOAGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating the accuracy and top-3 accuracy\n",
        "def count_correct(lst):\n",
        "    first = 0\n",
        "    top_3 = 0\n",
        "\n",
        "    for num in lst:\n",
        "        # Count the number of occurrences of digit 1\n",
        "        if num == 1:\n",
        "          first += 1\n",
        "\n",
        "        # Count the number of values between 2 and 3 (inclusive)\n",
        "        if 2 <= num <= 3:\n",
        "            top_3 += 1\n",
        "\n",
        "    return first, top_3+first\n"
      ],
      "metadata": {
        "id": "eiLCXb-TJJSP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BrainLeak V1"
      ],
      "metadata": {
        "id": "AeXBeuRzfywi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Inverted Samples\n",
        "Inv_BL1 = torch.load(\"MNIST_SNN_Inverted_BL1\",map_location=device)"
      ],
      "metadata": {
        "id": "6E64A9iSR1h6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attack Evaluation\n",
        "\n",
        "conf_mat = F.softmax(forward_pass(net,num_steps,Inv_BL1.permute(1, 0, 2,3).view([num_steps,10,1,28,28])),dim=2).mean(0) # Confidence Matrix\n",
        "rank_mat = torch.argsort(conf_mat,descending=True) # Ranking Matrix\n",
        "conf_ranks=[]\n",
        "\n",
        "for i in range(10):\n",
        "   conf_ranks.append(torch.where(rank_mat[i]==i)[0].item()+1)\n",
        "\n",
        "DAA_ranks = conf_mat.argmax(0) # Out of all inverted samples, which one is the most similar to digit X\n",
        "\n",
        "\n",
        "print(\"\\n ** Predicted Labels  :\\n\\n\",conf_mat.max(1)[1])\n",
        "print(\"\\n\\n ** Predicted Labels Confidence  :\\n\\n\",conf_mat.max(1)[0])\n",
        "\n",
        "print(\"\\n\\n\\n ** Ground Truth Ranks in Prediction  :\\n\\n\",conf_ranks)\n",
        "print(\"\\n\\n\\n ** Number of corrects (First,Top-3)  :\\n\\n\",count_correct(conf_ranks))\n",
        "\n",
        "print(\"\\n\\n ** Confidences of the correct outputs  :\\n\\n\",torch.diag(conf_mat))\n",
        "\n",
        "print(\"\\n\\n ** DAA Ranks  :\\n\\n\",DAA_ranks) # Out of all inverted samples, which one is the most similar to digit X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGqGuYJLJxpZ",
        "outputId": "b1992e7e-db7e-4cdc-9c15-f808c5d46c0c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ** Predicted Labels  :\n",
            "\n",
            " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
            "\n",
            "\n",
            " ** Predicted Labels Confidence  :\n",
            "\n",
            " tensor([0.9835, 0.7286, 0.9973, 0.9925, 0.9799, 0.9915, 0.9702, 0.9913, 0.9135,\n",
            "        0.7624], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "\n",
            "\n",
            "\n",
            " ** Ground Truth Ranks in Prediction  :\n",
            "\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "\n",
            "\n",
            " ** Number of corrects (First,Top-3)  :\n",
            "\n",
            " (10, 10)\n",
            "\n",
            "\n",
            " ** Confidences of the correct outputs  :\n",
            "\n",
            " tensor([0.9835, 0.7286, 0.9973, 0.9925, 0.9799, 0.9915, 0.9702, 0.9913, 0.9135,\n",
            "        0.7624], device='cuda:0', grad_fn=<DiagonalBackward0_copy>)\n",
            "\n",
            "\n",
            " ** DAA Ranks  :\n",
            "\n",
            " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Attack Accracy:\",count_correct(conf_ranks)[0]/10)\n",
        "print(\"\\n\\nTop-3 Attack Accuracy:\", count_correct(conf_ranks)[1]/10)\n",
        "print(\"\\n\\n ** Average Confidence of correct outputs\",torch.diag(conf_mat).mean().item())\n",
        "print(\"\\n\\nDAA:\", (sum(DAA_ranks==torch.tensor(range(10)).to(device)).item())/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W5Bjzi6K4in",
        "outputId": "6d87c863-d4e3-4554-c4be-ef3d8e17c8ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Accracy: 1.0\n",
            "\n",
            "\n",
            "Top-3 Attack Accuracy: 1.0\n",
            "\n",
            "\n",
            " ** Average Confidence of correct outputs 0.9310759902000427\n",
            "\n",
            "\n",
            "DAA: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brain Leak V2"
      ],
      "metadata": {
        "id": "W5SwSyZCf77z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Inverted Samples\n",
        "Inv_BL2 = torch.load(\"MNIST_SNN_Inverted_BL2\",map_location=device)"
      ],
      "metadata": {
        "id": "Y69MUtToLL4r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attack Evaluation\n",
        "\n",
        "\n",
        "conf_mat = F.softmax(forward_pass(net,num_steps,torch.bernoulli(Inv_BL2).permute(1, 0, 2,3).view([num_steps,10,1,28,28])),dim=2).mean(0) # Confidence Matrix\n",
        "rank_mat = torch.argsort(conf_mat,descending=True) # Ranking Matrix\n",
        "conf_ranks=[]\n",
        "\n",
        "for i in range(10):\n",
        "   conf_ranks.append(torch.where(rank_mat[i]==i)[0].item()+1)\n",
        "\n",
        "DAA_ranks = conf_mat.argmax(0) # Out of all inverted samples, which one is the most similar to digit X\n",
        "\n",
        "print(\"\\n ** Predicted Labels  :\\n\\n\",conf_mat.max(1)[1])\n",
        "print(\"\\n\\n ** Predicted Labels Confidence  :\\n\\n\",conf_mat.max(1)[0])\n",
        "\n",
        "print(\"\\n\\n\\n ** Ground Truth Ranks in Prediction  :\\n\\n\",conf_ranks)\n",
        "print(\"\\n\\n\\n ** Number of corrects (First,Top-3)  :\\n\\n\",count_correct(conf_ranks))\n",
        "\n",
        "print(\"\\n\\n ** Confidences of the correct outputs  :\\n\\n\",torch.diag(conf_mat))\n",
        "\n",
        "print(\"\\n\\n ** DAA Ranks  :\\n\\n\",DAA_ranks) # Out of all inverted samples, which one is the most similar to digit X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98iyf2dTLL7K",
        "outputId": "9a011efb-dce1-4bdd-db70-98e847cc50d0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ** Predicted Labels  :\n",
            "\n",
            " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
            "\n",
            "\n",
            " ** Predicted Labels Confidence  :\n",
            "\n",
            " tensor([0.9247, 0.9034, 0.9920, 0.9656, 0.9417, 0.9969, 0.9898, 0.9951, 0.9545,\n",
            "        0.7955], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "\n",
            "\n",
            "\n",
            " ** Ground Truth Ranks in Prediction  :\n",
            "\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "\n",
            "\n",
            " ** Number of corrects (First,Top-3)  :\n",
            "\n",
            " (10, 10)\n",
            "\n",
            "\n",
            " ** Confidences of the correct outputs  :\n",
            "\n",
            " tensor([0.9247, 0.9034, 0.9920, 0.9656, 0.9417, 0.9969, 0.9898, 0.9951, 0.9545,\n",
            "        0.7955], device='cuda:0', grad_fn=<DiagonalBackward0_copy>)\n",
            "\n",
            "\n",
            " ** DAA Ranks  :\n",
            "\n",
            " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Attack Accracy:\",count_correct(conf_ranks)[0]/10)\n",
        "print(\"\\n\\nTop-3 Attack Accuracy:\", count_correct(conf_ranks)[1]/10)\n",
        "print(\"\\n\\n ** Average Confidence of correct outputs\",torch.diag(conf_mat).mean().item())\n",
        "print(\"\\n\\nDAA:\", (sum(DAA_ranks==torch.tensor(range(10)).to(device)).item())/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6euwHSNKLL9v",
        "outputId": "0a07621f-3e82-482a-814f-4423339c1b10"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Accracy: 1.0\n",
            "\n",
            "\n",
            "Top-3 Attack Accuracy: 1.0\n",
            "\n",
            "\n",
            " ** Average Confidence of correct outputs 0.945917546749115\n",
            "\n",
            "\n",
            "DAA: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3RGfknyL058"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}